Today's topic: Components and Pipeline Implementation

Yesterday we've completed model_training.ipynb file.

Today, we'll try to do Modular coding, we'll see how we can convert that Jupyter notebook into modular coding i.e, how we can represent that Jupyter notebook in a modular style/way.

So far we've kept out EDA.ipynb and model_training.ipynb files inside the 'notebooks' folder.

Now, we'll see how to keep this model_training.ipynb in our pipeline(src folder).
Inside the src folder there are furthermore folders like components, pipeline, etc.
Inside components we have:
	data_ingestion.py
	data_transformation.py
	model_trainer.py

Today, we'll write the code in .py file and do modular coding and we'll try to connect each and every component all together i.e, 
data_ingestion.py component to data_transformation.py and that to model_trainer.py


Before we get started for today, lets do a quick recap of the previous session...

Pipeline is a connection of a number of steps. One of the steps is Feature Engineering.
Within FE there are a few steps:
	(i) Handling missing Values
	(ii) Handle the outliers
	(iii) Scale the Values
	(iv) Encode the Values 

Now, instead of writing a manual code of each and every step, we used scikit-learn.
Inside scikit-learn there is a class called Pipeline. We created an object of the class Pipeline and in whatever manner that we want to execute all these steps in the Feature Engineering we will do so. 

All those steps of FE we can write inside the Pipeline object and then will be executed in the same order. So yesterday we created the object of pipeline and mentioned if we have some missing values, handle them; if we have categorical data, perform the encoding; if we have to scale the data then do so; this was the pipeline created; now how can we execute the pipeline ?

There's another class named CloumnTransformer. We just need to keep the previously created class Pipeline(along with the data) inside the class ColumnTransformer.

Lets say we have created two pipelines:
	(i) Categorical Pipeline(where we'll keep the categorical data)
	(ii) Numerical Pipeline(where we'll keep the numerical data)

When we create the object of the CloumnTransformer(with all the above data and pipeline inside) and then if we perform 'fit_transform' automatically this pipeline will be executed.


Now lets get started with the modular coding...
First we have to activate the environment through bash(if not done so).

As we talked about earlier, we have created several files and folders inside the src folder:
Inside components we have:
	data_ingestion.py
	data_transformation.py
	model_trainer.py

Similarly inside pipelines we have:
	__init__.py
	prediction_pipeline.py
	training_pipeline.py

Similarly inside utils we have:
	__init__.py

And for the src folder itself we have:
	__init__.py
	exception.py
	logger.py

And many more files...

As we know that the Jupyter notebook which comes in ipynb file format is just for analysis and experimentation, But we cannot productionize this code i.e, the ipynb file is not for production, just for analysis and experimentation. 

We cannot deploy the Model we've created inside ipynb file format on GCP/Azure/AWS as its not for production i.e, its not productionizable. We can't even write a modular code inside the Jupyter notebook. 

A Modular code is where we segregate a task into different modules. In Python a .py file is called a Module.

Lets get started with components and within that data_ingestion.py
Rest of the work will be done and notes will be taken from there itself.


Q. Why is the logger.log file used in a Project ?
Ans
A logger file is used for logging each and every information. Lets say we've ingested our data from some csv file at 5:00 pm and I want to log this info.Now I want to transform the data but first from 5pm to 6pm the EDA and FE were done. All this information will be logged. 


Q. What is exceptions.py file ? 
Ans
Inside the exceptions.py file, we'll create our own custom exception class. For e.g, ZeroDivisionException, ArithmeticException, SyntaxError etc. In Python, we can create our own custom class for Exceptions Handling.


Note: Writing the code is not enough, after writing the code we should know how to do its debugging. Lets say we're getting an error while writing a code, then how can/will we get to know what the error is and what type of error it is ? 
Therefore debugging is very important and exceptions.py will help us in knowing in which line of code we're getting what error.

Q. What is utils file ?
Ans
Whether we're doing development in Python or ML or Web Development or any other development project structure(on a low level) we'll find utils.py file also. This is called a helper file/entity/module.


Now lets get started with the logger file...
I am facing some issues with the logger file so lets move on to exceptions.py for now...

Use ChatGPT to understand the scripts inside exceptions.py and logger.py files.


Now lets know about the concepts of the 'Artifacts and Configuration' where we're using dataclasses.


Whenever we're talking about any component like data_ingestion, data_transformation, model_training.
The output of data_ingestion is data/raw data, after getting raw data we're two more dataframes i.e, train and test data.
The output of data_transformation will be systematic data, data ready for model, modified data.
Q. Will I get a pre-processed object out of data_transformation ?
Ans. Yes 
The output of model_training will be the model itself.

All these outputs we'll get out of data_ingestion, data_transformation and model_training will be physical outputs.

In Machine Learning all these physical outputs are called 'Artifacts'. In every ML project and every repository we'll find out an Artifacts folder.


The complete component in the data_ingestion.py file with the Artifacts and everything else is finished; Now we'll test it.

Now, where will we collect all the components from data_ingestion, data_transformation and model_training ? 

Ans. In the Pipeline folder. 

We'll use training_pipeline.py to assemble all the components. Lets start coding there...

